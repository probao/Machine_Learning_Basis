###############
####线性模型####
###############



# coding: utf-8

# In[1]:

#######################
######线性回归#########
#######################


# In[2]:

#线性回归模型
# 对于这个例子的解释http://blog.csdn.net/zhangyingchengqi/article/details/54809401
# diabetes 是一个关于糖尿病的数据集， 该数据集包括442个病人的生理数据及一年以后的病情发展情况。   
# 数据集中的特征值总共10项, 如下:  
    # 年龄  
    # 性别  
    #体质指数  
    #血压  
    #s1,s2,s3,s4,s4,s6  (六种血清的化验数据)  
    #但请注意，以上的数据是经过特殊处理， 10个数据中的每个都做了均值中心化处理，然后又用标准差乘以个体数量调整了数值范围。验证就会发现任何一列的所有数值平方和为1.   
      
#关于数据集更多的信息: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html  
   # http://scikit-learn.org/stable/datasets/index.html#datasets


# In[3]:

from sklearn import datasets
diabetes = datasets.load_diabetes()


# In[10]:

diabetes.data[0]

array([ 0.03807591,  0.05068012,  0.06169621,  0.02187235, -0.0442235 ,
       -0.03482076, -0.04340085, -0.00259226,  0.01990842, -0.01764613])

# In[12]:

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model, discriminant_analysis, cross_validation


# In[16]:

# 这里给出加载数据集的函数
def load_data():
    diabetes =datasets.load_diabetes()
    return cross_validation.train_test_split(diabetes.data, diabetes.target, test_size=0.25, random_state=0)
#返回值：一个元组，元组依次是：训练样本集、测试样本集、训练样本集对应的标签值、测试样本集对应的标签值


# In[17]:

# 使用LinearRegression的函数如下：
def test_LinearRegression(*data):
    X_train,X_test,y_train,y_test=data
    regr = linear_model.LinearRegression()
    regr.fit(X_train, y_train)
    print('Coefficients:%s, intercept %.2f'%(regr.coef_, regr.intercept_))
    print("Residual sum of squares: %.2f" % np.mean((regr.predict(X_test)-y_test)**2))
    print('Score: %.2f'% regr.score(X_test, y_test))


# In[18]:

X_train, X_test, y_train, y_test=load_data()
test_LinearRegression(X_train, X_test, y_train, y_test)

Coefficients:[ -43.26774487 -208.67053951  593.39797213  302.89814903 -560.27689824
  261.47657106   -8.83343952  135.93715156  703.22658427   28.34844354], intercept 153.07
Residual sum of squares: 3180.20
Score: 0.36


# In[19]:

##########################
###线性回归模型的正则化###
##########################


# In[20]:

#岭回归
def test_Ridge(*data):  # *表示输入参数表示为元组
    X_train, X_test, y_train, y_test = data
    regr = linear_model.Ridge()
    regr.fit(X_train, y_train)
    print('Coefficients: %s, intercept %.2f' % (regr.coef_, regr.intercept_))
    print('Residual sum of squares: %.2f' % np.mean((regr.predict(X_test)-y_test)**2))
    print('Score: %.2f' % regr.score(X_test, y_test))


# In[21]:

X_train, X_test, y_train, y_test = load_data()
test_Ridge(X_train, X_test, y_train, y_test)


Coefficients: [  21.19927911  -60.47711393  302.87575204  179.41206395    8.90911449
  -28.8080548  -149.30722541  112.67185758  250.53760873   99.57749017], intercept 152.45
Residual sum of squares: 3192.33
Score: 0.36

# In[22]:

# 检测不同的alpha值对于预测性能的影响，给出测试函数
def test_Ridge_alpha(*data):
    X_train, X_test, y_train, y_test = data
    alphas=[0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]
    scores=[]
    for i,alpha in enumerate(alphas):
        regr = linear_model.Ridge(alpha=alpha)
        regr.fit(X_train, y_train)
        scores.append(regr.score(X_test, y_test))
    #绘图
    fig=plt.figure()
    ax=fig.add_subplot(1, 1, 1)
    ax.plot(alphas, scores)
    ax.set_xlabel(r"$\alpha$")
    ax.set_ylabel(r"score")
    ax.set_xscale('log')
    ax.set_title("Ridge")
    plt.show()


# In[23]:

# 调用函数
X_train, X_test, y_train, y_test = load_data()
test_Ridge_alpha(X_train, X_test, y_train, y_test)

#################
####Lasso回归####
#################
def test_Lasso(*data):
    X_train, X_test, y_train, y_test = data
    regr = linear_model.Lasso()
    regr.fit(X_train, y_train)
    print('Coefficients: %s, intercept %.2f' % (regr.coef_, regr.intercept_))
    print("Residual sum of squares: %.2f" % np.mean((regr.predict(X_test)-y_test)**2))
    print('Score: %.2f' % regr.score(X_test, y_test))

X_train, X_test, y_train, y_test = load_data()
test_Lasso(X_train, X_test, y_train, y_test)

Coefficients: [   0.           -0.          442.67992538    0.            0.            0.
   -0.            0.          330.76014648    0.        ], intercept 152.52
Residual sum of squares: 3583.42
Score: 0.28


# 下面检验不同的alpha对于预测性能的影响，给出测试函数
def test_Lasso_alpha(*data):
    X_train, X_test, y_train, y_test = data
    alphas = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]
    scores = []
    for i, alpha in enumerate(alphas):
        regr = linear_model.Lasso(alpha=alpha)
        regr.fit(X_train, y_train)
        scores.append(regr.score(X_test, y_test))
#绘图
    fig = plt.figure()
    ax = fig.add_subplot(1,1,1)
    ax.plot(alphas, scores)
    ax.set_xlabel(r"$\alpha$")
    ax.set_ylabel(r"score")
    ax.set_xscale('log')
    ax.set_title("Lasso")
    plt.show()

X_train, X_test, y_train, y_test = load_data()
test_Lasso_alpha(X_train, X_test, y_train, y_test)


#######################
#### ElasticNet回归####
#######################
# ElasticNet回归是对Lasso回归和岭回归的融合，其惩罚项是L1范数和L2范数的一个权衡
#使用ElasticNet的函数
def test_ElasticNet(*data):
    X_train, X_test, y_train, y_test = data
    regr = linear_model.ElasticNet()
    regr.fit(X_train, y_train)
    print('Coefficients: %s, intercept %.2f' % (regr.coef_, regr.intercept_))
    print("Residual sum of squares: %.2f" % np.mean((regr.predict(X_test)-y_test)**2))
    print('Score: %.2f' % regr.score(X_test, y_test))

X_train, X_test, y_train, y_test = load_data()
test_ElasticNet(X_train, X_test, y_train, y_test)

Coefficients: [ 0.40560736  0.          3.76542456  2.38531508  0.58677945  0.22891647
 -2.15858149  2.33867566  3.49846121  1.98299707], intercept 151.93
Residual sum of squares: 4922.36
Score: 0.01

# 下面检验不同的alpha，rho值对于预测性能的影响，给出测试函数
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm

def test_ElasticNet_alpha_rho(*data):
    X_train, X_test, y_train, y_test = data
    alphas = np.logspace(-2, 2)
    rhos = np.linspace(0.01, 1)
    scores=[]
    for alpha in alphas:
        for rho in rhos:
            regr = linear_model.ElasticNet(alpha=alpha, l1_ratio=rho)
            regr.fit(X_train, y_train)
            scores.append(regr.score(X_test, y_test))
# 绘图
    alphas, rhos = np.meshgrid(alphas, rhos)
    scores = np.array(scores).reshape(alphas.shape)
    fig = plt.figure()
    ax=Axes3D(fig)
    surf = ax.plot_surface(alphas, rhos, scores, rstride=1, cstride=1, cmap=cm.jet, linewidth=0, antialiased=False)
    fig.colorbar(surf, shrink=0.5, aspect=5)
    ax.set_xlabel(r"$\alpha$")
    ax.set_ylabel(r"$\rho$")
    ax.set_zlabel("score")
    ax.set_title("ElasticNet")
    plt.show()

X_train, X_test, y_train, y_test = load_data()
test_ElasticNet_alpha_rho(X_train, X_test, y_train, y_test)


#逻辑回归
#对鸢尾花进行分类

def load_data():
    iris = datasets.load_iris()
    X_train = iris.data
    y_train = iris.target
    return cross_validation.train_test_split(X_train, y_train, test_size=0.25, random_state=0, stratify=y_train) # stratify分层采样


def test_LogisticRegression(*data):
    X_train, X_test, y_train, y_test = data
    regr = linear_model.LogisticRegression()
    regr.fit(X_train, y_train)
    print('Coefficient: %s, intercept %s' % (regr.coef_, regr.intercept_))
    print('Score: %.2f' % regr.score(X_test, y_test))


# 调用函数
X_train, X_test, y_train, y_test = load_data()
test_LogisticRegression(X_train, X_test, y_train, y_test)

Coefficient: [[ 0.39310895  1.35470406 -2.12308303 -0.96477916]
 [ 0.22462128 -1.34888898  0.60067997 -1.24122398]
 [-1.50918214 -1.29436177  2.14150484  2.2961458 ]], intercept [ 0.24122458  1.13775782 -1.09418724]
Score: 0.97

#考察multi_class参数对分类结果的影响。默认采用的是one-vs-rest策略，但是逻辑回归模型
#原生就支持多类分类，给出的测试函数
def test_LogisticRegression_multinomial(*data):
    X_train, X_test, y_train, y_test = data
    regr = linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs') # 只有solver为牛顿法或者拟牛顿法才能配合multi_class='multinomial',否则报错
    regr.fit(X_train, y_train)
    print('Coefficient: %s, intercept %s' % (regr.coef_, regr.intercept_))
    print('Score: %.2f' % regr.score(X_test, y_test))

X_train, X_test, y_train, y_test = load_data()
test_LogisticRegression_multinomial(X_train, X_test, y_train, y_test)

Coefficient: [[-0.3844727   0.85502928 -2.27213058 -0.98537601]
 [ 0.34382763 -0.37438361 -0.03057819 -0.86096607]
 [ 0.04064507 -0.48064567  2.30270877  1.84634208]], intercept [  8.80428497   2.46960885 -11.27389382]
Score: 1.00


# 考察参数C对分类模型的预测性能的影响。C是正则化的倒数
# 它越小则正则化项的权重越大
def test_LogisticRegression_C(*data):
    X_train, X_test, y_train, y_test = data
    Cs = np.logspace(-2, 4, num=100)
    scores = []
    for C in Cs:
        regr = linear_model.LogisticRegression(C=C)
        regr.fit(X_train, y_train)
        scores.append(regr.score(X_test, y_test))
# 绘图
    fig = plt.figure()
    ax=fig.add_subplot(1, 1, 1)
    ax.plot(Cs, scores)
    ax.set_xlabel(r"C")
    ax.set_ylabel(r"score")
    ax.set_xscale('log')
    ax.set_title("LogisticRegression")
    plt.show()

X_train, X_test, y_train, y_test = load_data()
test_LogisticRegression_C(X_train, X_test, y_train, y_test)


##################
###线性判别分析###
##################

#线性判别分析的基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果

# zip函数
colors = 'rgb'
markers = 'o*s'
a=[0, 1, 2]
zip(a, colors, markers)
[(0, 'r', 'o'), (1, 'g', '*'), (2, 'b', 's')]


# ravel函数
#由ravel()展平的数组元素
a=np.array([[0,1,2],[3, 4, 5]])
a.ravel()
array([0, 1, 2, 3, 4, 5])


# numpy.vstack()函数
a = np.array([1, 2, 3])
b = np.array([2, 3, 4])
np.vstack((a, b))
array([[1, 2, 3],
       [2, 3, 4]])

np.hstack((a,b))
array([1, 2, 3, 2, 3, 4])

# 先给出使用LinearDiscriminantAnalysis
def test_LinearDiscriminantAnalysis(*data):
    X_train, X_test, y_train, y_test = data
    lda = discriminant_analysis.LinearDiscriminantAnalysis()
    lda.fit(X_train, y_train)
    print('Coefficients: %s, intercept %s' %(lda.coef_, lda.intercept_))
    print('Score: %.2f' % lda.score(X_test, y_test))

X_train, X_test, y_train, y_test = load_data()
test_LinearDiscriminantAnalysis(X_train, X_test, y_train, y_test)

Coefficients: [[  6.575853     9.75807593 -14.34026669 -21.39076537]
 [ -1.98385061  -3.49791089   4.21495042   2.60304299]
 [ -4.47116022  -6.09542385   9.85886057  18.29330864]], intercept [-15.33097142   0.46730077 -30.53297367]
Score: 1.00

# 现在来检查一下原始数据集在经过线性判别分析LDA之后的数据集的情况
# 给出绘制LDA降维之后的数据集的函数
from mpl_toolkits.mplot3d import Axes3D

def plot_LDA(converted_X, y):
    fig = plt.figure()
    ax = Axes3D(fig)
    colors = 'rgb'
    markers= 'o*s'
    for target, color, marker in zip([0, 1, 2], colors, markers):
        pos=(y==target).ravel()
        X=converted_X[pos, :]
        ax.scatter(X[:,0],X[:,1],X[:,2], color=color, marker=marker,
                  label="Label %d"%target) # X[:, 0] 第0列 X[:, 1] 第1列
    ax.legend(loc="best")
    fig.suptitle("Iris After LDA")
    plt.show()


    X_train, X_test, y_train, y_test = load_data()
X = np.vstack((X_train, X_test))
Y = np.vstack((y_train.reshape(y_train.size, 1), y_test.reshape(y_test.size, 1)))
lda = discriminant_analysis.LinearDiscriminantAnalysis()
lda.fit(X, Y)
converted_X = np.dot(X, np.transpose(lda.coef_))+lda.intercept_  # 第一项中的每一列加上lda.intercept
plot_LDA(converted_X, Y)


# 接下来考察不同的solver对预测性能的影响
def test_LinearDiscriminantAnalysis_solver(*data):
    X_train, X_test, y_train, y_test = data
    solvers = ['svd', 'lsqr', 'eigen']
    for solver in solvers:
        if(solver=='svd'):
            lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver)
        else:
            lda = discriminant_analysis.LinearDiscriminantAnalysis(solver=solver, shrinkage=None)
        lda.fit(X_train, y_train)
        print('Score at solver= %s: %.2f' %(solver, lda.score(X_test, y_test)))
    
# 调用该函数
X_train, X_test, y_train, y_test = load_data()
test_LinearDiscriminantAnalysis_solver(X_train, X_test, y_train, y_test)
Score at solver= svd: 1.00
Score at solver= lsqr: 1.00
Score at solver= eigen: 1.00


# 最后考察在solver=lsqr中引入抖动。引入抖动相当于引入正则化项
def test_LinearDiscriminantAnalysis_shrinkage(*data):
    X_train, X_test, y_train, y_test = data
    shrinkages = np.linspace(0.0, 1.0, num=20)
    scores = []
    for shrinkage in shrinkages:
        lda = discriminant_analysis.LinearDiscriminantAnalysis(solver='lsqr', shrinkage=shrinkage)
        lda.fit(X_train, y_train)
        scores.append(lda.score(X_test, y_test))
# 绘图
    fig = plt.figure()
    ax = fig.add_subplot(1, 1, 1)
    ax.plot(shrinkages, scores)
    ax.set_xlabel(r"shrinkage")
    ax.set_ylabel(r"score")
    ax.set_ylim(0, 1.05)
    ax.set_title("LinearDiscriminantAnalysis")
    plt.show()

X_train, X_test, y_train, y_test = load_data()
test_LinearDiscriminantAnalysis_shrinkage(X_train, X_test, y_train, y_test)








